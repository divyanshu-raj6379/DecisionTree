{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Gini Impurity: it's a measure of the purity of a split\n",
        "$G(\\mathcal{S}) = 1-\\sum_{i \\in \\mathcal {C}}p_{i}^2$, where $\\mathcal {C}$ is set of classes in dataset $\\mathcal{S}$ "
      ],
      "metadata": {
        "id": "9SZjirc8oOhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Entropy: It is a measure of the amount of uncertainty in the (data) set $\\mathcal{S}$ \n",
        "$H(\\mathcal{S})=-\\sum_{i\\in \\mathcal {C}}p_{i}\\log_{2}(p_{i})$, where $\\mathcal {C}$ is set of classes in dataset $\\mathcal{S}$"
      ],
      "metadata": {
        "id": "XJf4ABZJqQwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Information Gain on an attribute $\\mathcal{A}$ of $\\mathcal{S}$ is the measure of the difference in entropy from before to after the set $\\mathcal{S}$ is split on the attribute $\\mathcal{A}$ into subsets $\\mathcal{S_{1}}$, $\\mathcal{S_{2}}$,$...$, $\\mathcal{S_{m}}$\n",
        "$IG(\\mathcal{S},\\mathcal{A})=H(\\mathcal{S})-\\sum_{j=1}^{m}H(\\mathcal{S}|\\mathcal{S}_{j})$, where $H(\\mathcal{S}|\\mathcal{S}_{j})=\\frac{\\mid\\mathcal{S}_{j}\\mid}{\\mid \\mathcal{S} \\mid}.H(\\mathcal{S}_{j})$"
      ],
      "metadata": {
        "id": "5FpEdOAFvbfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ID3 Algorithm - \n",
        "ID3 stands for Iterative Dichotomiser 3 and is named such because the algorithm iteratively (repeatedly) dichotomizes(divides) features into two or more groups at each step.\n",
        "\n",
        "Invented by Ross Quinlan, ID3 uses a top-down greedy approach to build a decision tree. In simple words, the top-down approach means that we start building the tree from the top and the greedy approach means that at each iteration we select the best feature at the present moment to create a node.\n",
        "\n",
        "Most generally ID3 is only used for classification problems with nominal features only.\n",
        "\n",
        "Steps:\\\n",
        "$***$\\\n",
        "$1.$ Calculate the Information Gain of each feature.\\\n",
        "$2.$ Considering that all rows donâ€™t belong to the same class, split the dataset $S$ into subsets using the feature for which the Information Gain is maximum.\\\n",
        "$3.$ Make a decision tree node using the feature with the maximum Information gain.\\\n",
        "$4.$ If all rows belong to the same class, make the current node as a leaf node with the class as its label.\\\n",
        "$5.$ Repeat for the remaining features until we run out of all features, or the decision tree has all leaf nodes."
      ],
      "metadata": {
        "id": "hRExuMjv2Hhu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LN-F3ewonLa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}